---
title: "Machine Learning Course Project 12/27/15"
author: "CSD"
date: "December 26, 2015"
output:
  html_document:
    keep_md: yes
    pandoc_args: [
      "+RTS", "-K64m",
      "-RTS"
    ]
---

In this report we develop a model to predict the way that a person completed a particular exercise. The ways they performed the exercise range from A - E.

First we load the caret library and the training and test data that was provided:

```{r}
library(caret)
library(lattice)
library(ggplot2)
training = read.csv("data/pml-training.csv", header=TRUE)
test = read.csv("data/pml-testing.csv", header=TRUE)


```

We have a lot of possible features from this data. Quick data exploration shows the # observations and distribution of classes for our training data from A - E:

```{r}
nrow(training)
```

```{r, echo=FALSE}
plot(training$classe)
```

We have a lot of training data and we need to do cross-validation, so we will break up our training data into a test (25%) and training set (75%) and then use the test set provided as cross-validation.

```{r}
inTrain = createDataPartition(y=training$classe, p=0.75, list=FALSE)
trainingData = training[inTrain,]
testData = training[-inTrain,]
```

Now that we have training and test data we will do some preliminary data exploration to see which fit a model. Running summary(trainingData) shows that there are a lot of features that are majority "NAs" so we will just remove those from consideration.

```{r}
colsToRemove <- c("stddev_yaw_forearm", "var_yaw_forearm", "avg_yaw_forearm", "var_pitch_forearm", "stddev_pitch_forearm", "avg_pitch_forearm", "var_roll_forearm", "stddev_roll_forearm", "avg_roll_forearm", "var_accel_forearm", "amplitude_pitch_forearm", "amplitude_roll_forearm", "min_pitch_forearm", "min_roll_forearm", "max_picth_forearm", "max_roll_forearm", "var_yaw_dumbbell", "stddev_yaw_dumbbell", "avg_yaw_dumbbell", "var_pitch_dumbbell", "stddev_pitch_dumbbell", "avg_pitch_dumbbell", "var_roll_dumbbell", "stddev_roll_dumbbell", "avg_roll_dumbbell", "var_accel_dumbbell", "amplitude_pitch_dumbbell", "amplitude_roll_dumbbell", "min_pitch_dumbbell", "min_roll_dumbbell", "max_picth_dumbbell", "max_roll_dumbbell", "amplitude_yaw_arm", "amplitude_pitch_arm", "amplitude_roll_arm", "min_yaw_arm", "min_pitch_arm", "min_roll_arm", "max_yaw_arm", "max_picth_arm", "max_roll_arm", "var_yaw_arm", "stddev_yaw_arm", "avg_yaw_arm", "var_pitch_arm", "stddev_pitch_arm", "avg_pitch_arm", "var_roll_arm", "stddev_roll_arm", "avg_roll_arm", "var_yaw_belt", "stddev_yaw_belt", "avg_yaw_belt", "var_pitch_belt", "stddev_pitch_belt", "avg_pitch_belt", "var_roll_belt", "stddev_roll_belt", "avg_roll_belt", "var_total_accel_belt", "amplitude_pitch_belt", "amplitude_roll_belt", "min_pitch_belt", "min_roll_belt", "max_picth_belt", "max_roll_belt")
trainingData = trainingData[,!(names(trainingData) %in% colsToRemove)]
```
Now we will see which features might be relevant. Let's look at just the "yaw" columns.

```{r}
names(trainingData)
featurePlot(x=trainingData[, c("yaw_belt", "yaw_arm", "yaw_dumbbell", "yaw_forearm")  ], y = trainingData$classe, plot="pairs")

```


Now we will make a model. 
```{r}
set.seed(1234)
#modelFit = train(classe ~., data=trainingData, method="glm")
#modelFit
```

Now that we have a model we will make our predictions for the test data.
```{r}
#predictions = predict(modelFit, newdata=testData)
#predictions
```

And we make a confusion matrix to see how accurate our predictions were and what our out of sample error rate is. 
```{r}
#confusionMatrix(predictions, testData$type)
```

This is having some errors but that is the basic idea as understood by a complete newbie to machine learning.